{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "933b59e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bee70989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aa0297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0e6171",
   "metadata": {},
   "source": [
    "# NLTK library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1f47f4",
   "metadata": {},
   "source": [
    "Various Python libraries are available for Natural Language Processing. Some of the most popular are NLTK, SpaCy and FastText.\n",
    "\n",
    "[NLTK](https://www.nltk.org/) (Natural Language ToolKit) is the one with the longest history and which offers the most functionality for working with text. It is used by linguists, researchers in the digital humanities, as well as researchers in the field of natural language processing. Library provides support for working with a large number of languages. The following are examples of some of the most important and commonly used methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f886a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29614b66",
   "metadata": {},
   "source": [
    "The text we will use for the demonstration is taken from a Wikipedia article about Nikola Tesla in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b634f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Born and raised in the Austrian Empire, Tesla studied engineering and physics in the 1870s without receiving a degree, gaining practical experience in the early 1880s working in telephony and at Continental Edison in the new electric power industry. In 1884 he emigrated to the United States, here he became a naturalized citizen. He worked for a short time at the Edison Machine Works in New York City before he struck out on his own. With the help of partners to finance and market his ideas, Tesla set up laboratories and companies in New York to develop a range of electrical and mechanical devices. His alternating current (AC) induction motor and related polyphase AC patents, licensed by Westinghouse Electric in 1888, earned him a considerable amount of money and became the cornerstone of the polyphase system which that company eventually marketed.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd81af3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Born and raised in the Austrian Empire, Tesla studied engineering and physics in the 1870s without receiving a degree, gaining practical experience in the early 1880s working in telephony and at Continental Edison in the new electric power industry. In 1884 he emigrated to the United States, here he became a naturalized citizen. He worked for a short time at the Edison Machine Works in New York City before he struck out on his own. With the help of partners to finance and market his ideas, Tesla set up laboratories and companies in New York to develop a range of electrical and mechanical devices. His alternating current (AC) induction motor and related polyphase AC patents, licensed by Westinghouse Electric in 1888, earned him a considerable amount of money and became the cornerstone of the polyphase system which that company eventually marketed.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d716be4e",
   "metadata": {},
   "source": [
    "## Splitting text into sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc10555",
   "metadata": {},
   "source": [
    "Support for advanced sentence-tokenization is provided through the <code>sent_tokenize()</code> function from <code>tokenize</code> module. Punctuation marks such as periods, question marks, or exclamation marks are used as separators. The function is able to distinguish the occurrences of these characters in other contexts and to associate them with the correct function. For example, a dot that is a part of the abbreviation U.S.A. or date 20.02.2020. will not affect on wrong splitting of a sentence.\n",
    "\n",
    "<div><br><span style=\"font-size:25px\">&#9888;</span> $\\hspace{0.1cm}$<code>sent_tokenize()</code> function internally uses a pre-trained model <code>PunktSentenceTokenizer</code> to detect the boundaries of sentences which is stored in the NLTK data module <code>punkt</code> and needs to be downloaded in order to use the function</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45ce28f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bfe73a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nevena/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c043eca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = tokenize.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bc69e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Born and raised in the Austrian Empire, Tesla studied engineering and physics in the 1870s without receiving a degree, gaining practical experience in the early 1880s working in telephony and at Continental Edison in the new electric power industry.',\n",
       " 'In 1884 he emigrated to the United States, here he became a naturalized citizen.',\n",
       " 'He worked for a short time at the Edison Machine Works in New York City before he struck out on his own.',\n",
       " 'With the help of partners to finance and market his ideas, Tesla set up laboratories and companies in New York to develop a range of electrical and mechanical devices.',\n",
       " 'His alternating current (AC) induction motor and related polyphase AC patents, licensed by Westinghouse Electric in 1888, earned him a considerable amount of money and became the cornerstone of the polyphase system which that company eventually marketed.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f95ff1",
   "metadata": {},
   "source": [
    "## Splitting sentences into words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c143b5",
   "metadata": {},
   "source": [
    "Sentences can be further divided into words (tokens). Word-tokenization is provided through the <code>word_tokenize()</code> function, also from the <code>tokenize</code> module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d65b7576",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.tokenize.word_tokenize(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2378db34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Born',\n",
       " 'and',\n",
       " 'raised',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Austrian',\n",
       " 'Empire',\n",
       " ',',\n",
       " 'Tesla',\n",
       " 'studied',\n",
       " 'engineering',\n",
       " 'and',\n",
       " 'physics',\n",
       " 'in',\n",
       " 'the',\n",
       " '1870s',\n",
       " 'without',\n",
       " 'receiving',\n",
       " 'a',\n",
       " 'degree',\n",
       " ',',\n",
       " 'gaining',\n",
       " 'practical',\n",
       " 'experience',\n",
       " 'in',\n",
       " 'the',\n",
       " 'early',\n",
       " '1880s',\n",
       " 'working',\n",
       " 'in',\n",
       " 'telephony',\n",
       " 'and',\n",
       " 'at',\n",
       " 'Continental',\n",
       " 'Edison',\n",
       " 'in',\n",
       " 'the',\n",
       " 'new',\n",
       " 'electric',\n",
       " 'power',\n",
       " 'industry',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1fc1b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c832a2",
   "metadata": {},
   "source": [
    "## Token filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66256790",
   "metadata": {},
   "source": [
    "It is obvious that some of the extracted tokens should be filtered-out. For a start, punctuation marks can be excluded. The <code>string</code> library defines constant <code>punctuation</code> that contains all punctuation marks and can be used to identify tokens that represent punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "392c0a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bc60e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f5b2e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_without_punctuation = [token for token in tokens if token not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "611fa9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Born',\n",
       " 'and',\n",
       " 'raised',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Austrian',\n",
       " 'Empire',\n",
       " 'Tesla',\n",
       " 'studied',\n",
       " 'engineering',\n",
       " 'and',\n",
       " 'physics',\n",
       " 'in',\n",
       " 'the',\n",
       " '1870s',\n",
       " 'without',\n",
       " 'receiving',\n",
       " 'a',\n",
       " 'degree',\n",
       " 'gaining',\n",
       " 'practical',\n",
       " 'experience',\n",
       " 'in',\n",
       " 'the',\n",
       " 'early',\n",
       " '1880s',\n",
       " 'working',\n",
       " 'in',\n",
       " 'telephony',\n",
       " 'and',\n",
       " 'at',\n",
       " 'Continental',\n",
       " 'Edison',\n",
       " 'in',\n",
       " 'the',\n",
       " 'new',\n",
       " 'electric',\n",
       " 'power',\n",
       " 'industry']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_without_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43ad8e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_without_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0f8511",
   "metadata": {},
   "source": [
    "Further, words that appear very often and therefore have little discriminative value can be excluded from the vocabulary. These words are called **stop words**. For example, in English these are articles 'a', 'an' and 'the', prepositions like 'in', 'at', 'on' and etc. Filtering criteria depends primarily on the specific problem to be solved and cannot be roughly generalized. The general strategy for determining a stop list is to sort words by frequency, and then to label the most frequent terms as a stop list.\n",
    "\n",
    "Common stop words for different languages are available in the NLTK library through a submodule <code>stopwords</code> of <code>corpus</code> module. The list of stop words for a specific language can be obtained by function <code>words()</code>.\n",
    "\n",
    "<div><br><span style=\"font-size:25px\">&#9888;</span> $\\hspace{0.1cm}$ in order to use <code>words()</code> function it is necessary to download stop word corpora from the NLTK data module <code>stopwords</code>!</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85b315ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa97a052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/nevena/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5462b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6f651f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "315b4648",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_without_stopwords = [token for token in tokens_without_punctuation if token not in stopwords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52957a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Born',\n",
       " 'raised',\n",
       " 'Austrian',\n",
       " 'Empire',\n",
       " 'Tesla',\n",
       " 'studied',\n",
       " 'engineering',\n",
       " 'physics',\n",
       " '1870s',\n",
       " 'without',\n",
       " 'receiving',\n",
       " 'degree',\n",
       " 'gaining',\n",
       " 'practical',\n",
       " 'experience',\n",
       " 'early',\n",
       " '1880s',\n",
       " 'working',\n",
       " 'telephony',\n",
       " 'Continental',\n",
       " 'Edison',\n",
       " 'new',\n",
       " 'electric',\n",
       " 'power',\n",
       " 'industry']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "068b24af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1333ff12",
   "metadata": {},
   "source": [
    "## Lemmatization and stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be61c0ad",
   "metadata": {},
   "source": [
    "After filtering, the tokens can be further processed. For instance, tokens 'works', 'worked' and 'working' could be reduced to the same form 'work' (the infinitive of the verb), and tokens 'ideas' and 'devices' to 'idea' and 'device' (singular nouns). Depending on whether the reduction of similar words to the same form is done according to linguistic rules or some heuristic rules, we are talking about **lemmatization** or **stemming**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd2b285",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f3213a",
   "metadata": {},
   "source": [
    "Lematization assigns the so-called **lemma** (grammatical root) to tokens. For example, verbs are assigned with a form in the infinitive, and nouns by a form in the singular.\n",
    "\n",
    "NLTK library offers support for lemmatization through class <code>WordNetLemmatizer</code> which relies on the manually created lexical database **WordNet** in which nouns, verbs and adjectives are grouped according to the same grammatical root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e1f4b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2df882",
   "metadata": {},
   "source": [
    "Lematization is performed using the <code>lemmatize()</code> method. Along with the token method expects the so-called POS tag (Part-Of-Speach Tag) - information about the type of word as an approximation of a token context. Possible values for the <code>pos</code> parameter are:\n",
    "- <code>'n'</code> for nouns\n",
    "- <code>'v'</code> for verbs\n",
    "- <code>'a'</code> for adjectives\n",
    "- <code>'r'</code> for adverbs\n",
    "- <code>'s'</code> for so-called satellite adjectives (denote an adjective that is in some sense satellite to its noun, relational adjectives, adjectives whose meaning depends on the context, ie the noun which they satellite to, e.g. 'atomic bomb' and 'atomic adjustments', where adjective 'atomic', according to the order, has meaning that the bomb is based on the release of energy due to the fission of atoms, ie that the change is extremely small, immeasurable).\n",
    "\n",
    "<div><br><span style=\"font-size:25px\">&#9888;</span> $\\hspace{0.1cm}$ in order to use the method <code>lemmatize()</code> it is necessary to download the WordNet database from the NLTK data module <code>wordnet</code>!</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82abc683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nevena/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "627df1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('working', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f620297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'working'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('working', pos='a')       #e.g. 'working class'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b35afa",
   "metadata": {},
   "source": [
    "Note that associated lemma differs whether (same) token appears as a verb or as an adjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0c2649b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'idea'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('ideas', pos='n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f12bcabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'atomic'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('atomic', pos='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e4f558f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arid'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('arid', pos='s')          #I: lacking water, dry, II: lacking vitality or spirit, lifeless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df5f941",
   "metadata": {},
   "source": [
    "Satellite attributes are usually not shortened, as their meaning depends on the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54041340",
   "metadata": {},
   "source": [
    "Automatic assignment of POS tags to tokens can be performed used so-called **POS taggers**. These are usually sequence labeling classifiers trained on a large collection of manually annotated texts. \n",
    "\n",
    "Support for automatized assignment of POS tags is provided through the <code>pos_tag()</code> function</code>. POS taggers by default use the PennTreebank tagset. In it, for example, the POS label for proper nouns is ‘NNP’, for nouns ‘NN’, for adjectives 'JJ', for prepositions ‘IN’, etc. Complete PennTreebank scheme can be found [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n",
    "\n",
    "<div><br><span style=\"font-size:25px\">&#9888;</span> $\\hspace{0.1cm}$<code>pos_tag()</code> function by default \n",
    "uses <code>AveragedPerceptron</code> POS tager (NLTK version 3.6.1) which is stored in the NLTK data module <code>averaged_perceptron_tagger</code> and needs to be downloaded in order to use the function</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09866a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/nevena/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4809791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Born', 'NNP'),\n",
       " ('raised', 'VBD'),\n",
       " ('Austrian', 'JJ'),\n",
       " ('Empire', 'NNP'),\n",
       " ('Tesla', 'NNP'),\n",
       " ('studied', 'VBD'),\n",
       " ('engineering', 'NN'),\n",
       " ('physics', 'NNS'),\n",
       " ('1870s', 'CD'),\n",
       " ('without', 'IN'),\n",
       " ('receiving', 'VBG'),\n",
       " ('degree', 'NN'),\n",
       " ('gaining', 'VBG'),\n",
       " ('practical', 'JJ'),\n",
       " ('experience', 'NN'),\n",
       " ('early', 'RB'),\n",
       " ('1880s', 'CD'),\n",
       " ('working', 'VBG'),\n",
       " ('telephony', 'NN'),\n",
       " ('Continental', 'NNP'),\n",
       " ('Edison', 'NNP'),\n",
       " ('new', 'JJ'),\n",
       " ('electric', 'JJ'),\n",
       " ('power', 'NN'),\n",
       " ('industry', 'NN')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokens_without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442ab5de",
   "metadata": {},
   "source": [
    "In order to use POS tagger in lemmatization, it is necessary to match the tags associated with the tokens. WordNet lematizer uses its more modest POS tagset - <code>'n'</code>, <code>'v'</code>, <code>'a'</code>, <code>'r'</code> correspond in order to list of PennTreebank POS tags \\['NN', 'NNS', 'NNP', 'NNPS'\\], \\['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'\\], \\['JJ', 'JJR', 'JJS'\\], \\['RB', 'RBR', 'RBS'\\], while satellite adjectives are treated the same as other adjectives, ie. PennTreebank does not have a special POS tag for them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8e578c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos_tag(token):\n",
    "    #pairing first character of the PennTreebank POS tag with the corresponding WordNet POS tag\n",
    "    pos_tag_dict = {\n",
    "        'N' : 'n',\n",
    "        'V' : 'v',\n",
    "        'J' : 'a',\n",
    "        'R' : 'r'\n",
    "    }\n",
    "    \n",
    "    penn_pos_tag = nltk.pos_tag([token])[0][1][0]\n",
    "    \n",
    "    #mapping to the corresponding WordNet POS tag or set as default to the noun POS tag\n",
    "    if penn_pos_tag in pos_tag_dict:\n",
    "        return pos_tag_dict[penn_pos_tag]\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3e9c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_tokens = []\n",
    "for token in tokens_without_stopwords:\n",
    "    pos_tag = get_wordnet_pos_tag(token)\n",
    "    lemmatized_tokens.append(lemmatizer.lemmatize(token, pos_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fff684ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Born',\n",
       " 'raise',\n",
       " 'Austrian',\n",
       " 'Empire',\n",
       " 'Tesla',\n",
       " 'study',\n",
       " 'engineering',\n",
       " 'physic',\n",
       " '1870s',\n",
       " 'without',\n",
       " 'receive',\n",
       " 'degree',\n",
       " 'gain',\n",
       " 'practical',\n",
       " 'experience',\n",
       " 'early',\n",
       " '1880s',\n",
       " 'work',\n",
       " 'telephony',\n",
       " 'Continental',\n",
       " 'Edison',\n",
       " 'new',\n",
       " 'electric',\n",
       " 'power',\n",
       " 'industry']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9576bda3",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b0828b",
   "metadata": {},
   "source": [
    "Stemming assigns the so-called **stem** (atrificial root, part of a word responsible for its lexical meaning) to tokens. For example, some common rules for stemming are truncating the 'ed' suffix (e.g. 'played' $\\rightarrow$ 'play'), replacing the suffix 'ational' with the suffix 'ate' (e.g. 'relational' $\\rightarrow$ 'relate') or replace the suffix 'ization' with the suffix 'ize' (e.g. 'organization' $\\rightarrow$ 'organize'). The stem doesn't need to be identical to grammatical root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. \n",
    "\n",
    "NLTK library offers support for steaming through class <code>PorterStemmer</code> which uses Porter's steaming algorithm, one of the most well-known English-language steaming algorithms. Some of other available stemmers are <code>SnowballStemmer</code>, <code>LancasterStemmer</code> i <code>RegexpStemmer</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "caf4d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e2eeb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18f3656c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'idea'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('ideas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "19596e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'atom'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('atomic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc12cc25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arid'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('arid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c191e4f6",
   "metadata": {},
   "source": [
    "Whether we will use lemmatization or stemming depends on the specific task we are solving, as well as the available resources. Lematization requires the existence of a lexical database similar to WordNet and it can significantly slow down the program, while stemming gives faster but less accurate results.\n",
    "\n",
    "Lemmatization and stemming are text normalization (or sometimes called word normalization) techniques that are used to prepare text for further processing. Depending on the type of textual content, other types of normalization techniques may be needed as well. For instance, language on social networks is often very specific, rich in abbreviations and words that deviate from the standard spelling  ('u2' is short for 'you too', 'tmrw' for 'tomorrow', and 'cooool' for 'cool').\n",
    "\n",
    "Previously described techniques perform text preprocessing and thereby implicitly build the vocabulary. Based on the constructed vocabulary we further define text representation that will be used for machine learning algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
