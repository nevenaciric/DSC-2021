{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac8f176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce699c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78ca8f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "399c1520",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5654452e",
   "metadata": {},
   "source": [
    "TSV file <code>'data/SMSSpamCollection.tsv'</code> contains a collection of SMS messages with associated labels\n",
    "- 'spam' - if the message is spam\n",
    "- 'ham' - if the message is not spam\n",
    "\n",
    "The task is to apply the presented techniques for preprocessing, perform feature extraction and make a classifier that distinguishes spam and non-spam messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011a9143",
   "metadata": {},
   "source": [
    "1) Load dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e993774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/SMSSpamCollection.tsv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44365248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                                                  1\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7e9d0e",
   "metadata": {},
   "source": [
    "2) Extract class labels and text corpus separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "460e609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[1]\n",
    "y = data[0].apply(lambda x : int(x == 'spam'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ffd445",
   "metadata": {},
   "source": [
    "3) Split dataset into a training and test subsets in ratio 2:1, with stratification (parameter <code>stratify</code>). Set <code>random_state</code> parameter to 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4f1eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, test_size=0.33, stratify=y, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25568291",
   "metadata": {},
   "source": [
    "4) Define a function which will be used for text tokenization and preprocessing (use all of the presented procedures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86cf35dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sms_stem_tokenizer(sms_text):\n",
    "    sms_tokens = nltk.tokenize.word_tokenize(sms_text)\n",
    "    \n",
    "    sms_stems = []\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    \n",
    "    for token in sms_tokens:\n",
    "        if token in string.punctuation:\n",
    "            continue\n",
    "        if token in stopwords.words('english'):\n",
    "            continue\n",
    "        if token.isdigit():\n",
    "            continue\n",
    "            \n",
    "        stem = stemmer.stem(token)\n",
    "        sms_stems.append(stem)\n",
    "        \n",
    "    return sms_stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "067669c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos_tag(token):\n",
    "    pos_tag_dict = {\n",
    "        'N' : 'n',\n",
    "        'V' : 'v',\n",
    "        'J' : 'a',\n",
    "        'R' : 'r'\n",
    "    }\n",
    "    \n",
    "    penn_pos_tag = nltk.pos_tag([token])[0][1][0]\n",
    "    \n",
    "    if penn_pos_tag in pos_tag_dict:\n",
    "        return pos_tag_dict[penn_pos_tag]\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62ff2a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sms_lemma_tokenizer(sms_text):\n",
    "    sms_tokens = nltk.tokenize.word_tokenize(sms_text)\n",
    "    \n",
    "    sms_lemmas = []\n",
    "    \n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    for token in sms_tokens:\n",
    "        if token in string.punctuation:\n",
    "            continue\n",
    "        if token in stopwords.words('english'):\n",
    "            continue\n",
    "        if token.isdigit():\n",
    "            continue\n",
    "            \n",
    "        pos_tag = get_wordnet_pos_tag(token)\n",
    "        lemma = lemmatizer.lemmatize(token, pos_tag)\n",
    "        sms_lemmas.append(lemma)\n",
    "        \n",
    "    return sms_lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078f3edb",
   "metadata": {},
   "source": [
    "5) Perform feature extraction using some of the presented methods. When building a vocabulary, exclude words that appear less than 2 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9400278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_1 = feature_extraction.text.CountVectorizer(min_df=2, tokenizer=sms_stem_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "495ffd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nevena/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(min_df=2,\n",
       "                tokenizer=<function sms_stem_tokenizer at 0x7f0cf55ac820>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer_1.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcc490d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vectorized_1 = count_vectorizer_1.transform(x_train)\n",
    "x_test_vectorized_1 = count_vectorizer_1.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51f6254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_2 = feature_extraction.text.CountVectorizer(min_df=2, tokenizer=sms_lemma_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1aefbeb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(min_df=2,\n",
       "                tokenizer=<function sms_lemma_tokenizer at 0x7f0cf44d5c10>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer_2.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0a4c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vectorized_2 = count_vectorizer_2.transform(x_train)\n",
    "x_test_vectorized_2 = count_vectorizer_2.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f810501",
   "metadata": {},
   "source": [
    "6) Take a look at the size of the built vocabulary. Compare it to the size of the vocabulary obtained without text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ead22fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2676"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_vectorizer_1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34694f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2708"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_vectorizer_2.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeddc6e6",
   "metadata": {},
   "source": [
    "Note that we got a much smaller vocabulary compared to the vocabulary obtained without the application of stemming/lemmatization which had 3209 words. This can be a good thing because reducing number of features also reduces the capacity of the model to memorize the training data, ie reduces the chance of overfitting. On the other hand, stemming as well as lemmatization may cause loss of information, or even changed meaning of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05305dae",
   "metadata": {},
   "source": [
    "7) Train several types of classification models, evaluate them and compare with the previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef13c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29734b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(x_train_vectorized_1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73114dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b264c3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x_train_vectorized_2, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74219a05",
   "metadata": {},
   "source": [
    "Accuracy of the model on the training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6a63910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.993302973479775"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.score(x_train_vectorized_1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fb222f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9798803697661773"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.score(x_test_vectorized_1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc110b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.993570854540584"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.score(x_train_vectorized_2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "585180a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9798803697661773"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.score(x_test_vectorized_2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6dcf81",
   "metadata": {},
   "source": [
    "Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76a9f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted = model1.predict(x_test_vectorized_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ef3a073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1590,    2],\n",
       "       [  35,  212]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07198ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted = model2.predict(x_test_vectorized_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "799390f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1589,    3],\n",
       "       [  34,  213]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_test_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2eb744",
   "metadata": {},
   "source": [
    "Results are slightly worse then in the case of the corresponding models trained on representations obtained without preprocessing and stemming/lemmatization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
